{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52d6a531",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scEMAIL_model import *\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d18e21",
   "metadata": {},
   "source": [
    "### load the preprocessed target data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e205d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataname: Pancreas\n"
     ]
    }
   ],
   "source": [
    "dataset='Pancreas'\n",
    "print(\"dataname:\", dataset)\n",
    "adata = sc.read(\"/data/wanh/scEMAIL/real_data/{}_adata.h5ad\".format(dataset))\n",
    "X = adata.X.astype(np.float32)\n",
    "with open('/data/wanh/scEMAIL/real_data/{}_count_X.csv'.format(dataset),newline='') as csvfile:\n",
    "    spamreader = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
    "    count_X = []\n",
    "    for row in spamreader:\n",
    "        count_X.append([round(float(j)) for j in row])\n",
    "count_X = np.array(count_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4461681",
   "metadata": {},
   "source": [
    "### load the pre-trained source model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cbca6134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source cell types: 9 ['acinar', 'alpha', 'beta', 'delta', 'ductal', 'endothelial', 'epsilon', 'gamma', 'mesenchymal']\n"
     ]
    }
   ],
   "source": [
    "checkpoint = torch.load(\"/data/wanh/scEMAIL/source_model/real_data/AE_weights_{}.pth.tar\".format(dataset))\n",
    "source_class_num = checkpoint['ae_state_dict'][\"classifier.0.bias\"].size()[0]\n",
    "model = target_model(input_dim=adata.n_vars, z_dim=32, n_clusters=source_class_num,\n",
    "                             encodeLayer=[256, 64], decodeLayer=[64, 256], sigma=2.5).cuda()\n",
    "model_dict = model.state_dict()\n",
    "for i in checkpoint['ae_state_dict']:\n",
    "    model_dict[i] = checkpoint['ae_state_dict'][i]\n",
    "model.load_state_dict(model_dict)\n",
    "\n",
    "with open('/data/wanh/scEMAIL/real_data/{}_annotation.csv'.format(dataset),newline='') as csvfile:\n",
    "    spamreader = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
    "    source_annotation = []\n",
    "    for row in spamreader:\n",
    "        source_annotation.append(row[0])\n",
    "print(\"source cell types:\",source_class_num, source_annotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc0c5319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of samples: 2282\n",
      "number of class: 9\n",
      "bimodality of dip test: 0.42115788421157885 False\n",
      "bimodality coefficient:(>0.555 indicates bimodality) 0.5071104284345349 False\n",
      "ood sample exists: False\n",
      "Pretrain epoch [1/1], ZINB loss:5.5391\n",
      "Pretrain epoch [2/1], ZINB loss:4.2187\n",
      "Pretrain epoch [3/1], ZINB loss:3.3590\n",
      "Pretrain epoch [4/1], ZINB loss:3.7244\n",
      "Pretrain epoch [5/1], ZINB loss:3.0431\n",
      "Pretrain epoch [6/1], ZINB loss:3.1818\n",
      "Pretrain epoch [7/1], ZINB loss:2.8888\n",
      "Pretrain epoch [8/1], ZINB loss:2.5971\n",
      "Pretrain epoch [9/1], ZINB loss:2.7450\n",
      "Pretrain epoch [1/2], ZINB loss:2.5437\n",
      "Pretrain epoch [2/2], ZINB loss:2.4650\n",
      "Pretrain epoch [3/2], ZINB loss:2.3449\n",
      "Pretrain epoch [4/2], ZINB loss:2.3353\n",
      "Pretrain epoch [5/2], ZINB loss:2.1993\n",
      "Pretrain epoch [6/2], ZINB loss:2.2195\n",
      "Pretrain epoch [7/2], ZINB loss:1.8883\n",
      "Pretrain epoch [8/2], ZINB loss:1.9429\n",
      "Pretrain epoch [9/2], ZINB loss:1.8747\n",
      "Pretrain epoch [1/3], ZINB loss:1.7869\n",
      "Pretrain epoch [2/3], ZINB loss:1.7559\n",
      "Pretrain epoch [3/3], ZINB loss:1.7834\n",
      "Pretrain epoch [4/3], ZINB loss:1.6566\n",
      "Pretrain epoch [5/3], ZINB loss:1.8138\n",
      "Pretrain epoch [6/3], ZINB loss:1.6688\n",
      "Pretrain epoch [7/3], ZINB loss:1.6620\n",
      "Pretrain epoch [8/3], ZINB loss:1.6410\n",
      "Pretrain epoch [9/3], ZINB loss:1.5623\n",
      "Pretrain epoch [1/4], ZINB loss:1.5827\n",
      "Pretrain epoch [2/4], ZINB loss:1.4552\n",
      "Pretrain epoch [3/4], ZINB loss:1.4469\n",
      "Pretrain epoch [4/4], ZINB loss:1.3364\n",
      "Pretrain epoch [5/4], ZINB loss:1.4157\n",
      "Pretrain epoch [6/4], ZINB loss:1.3886\n",
      "Pretrain epoch [7/4], ZINB loss:1.4060\n",
      "Pretrain epoch [8/4], ZINB loss:1.4218\n",
      "Pretrain epoch [9/4], ZINB loss:1.3742\n",
      "Pretrain epoch [1/5], ZINB loss:1.3136\n",
      "Pretrain epoch [2/5], ZINB loss:1.2772\n",
      "Pretrain epoch [3/5], ZINB loss:1.3164\n",
      "Pretrain epoch [4/5], ZINB loss:1.2510\n",
      "Pretrain epoch [5/5], ZINB loss:1.2490\n",
      "Pretrain epoch [6/5], ZINB loss:1.1861\n",
      "Pretrain epoch [7/5], ZINB loss:1.3231\n",
      "Pretrain epoch [8/5], ZINB loss:1.2955\n",
      "Pretrain epoch [9/5], ZINB loss:1.1950\n",
      "Pretrain epoch [1/6], ZINB loss:1.1975\n",
      "Pretrain epoch [2/6], ZINB loss:1.1113\n",
      "Pretrain epoch [3/6], ZINB loss:1.1391\n",
      "Pretrain epoch [4/6], ZINB loss:1.1145\n",
      "Pretrain epoch [5/6], ZINB loss:1.1696\n",
      "Pretrain epoch [6/6], ZINB loss:1.1552\n",
      "Pretrain epoch [7/6], ZINB loss:1.2649\n",
      "Pretrain epoch [8/6], ZINB loss:1.1417\n",
      "Pretrain epoch [9/6], ZINB loss:1.1477\n",
      "Pretrain epoch [1/7], ZINB loss:1.0961\n",
      "Pretrain epoch [2/7], ZINB loss:1.1076\n",
      "Pretrain epoch [3/7], ZINB loss:1.0390\n",
      "Pretrain epoch [4/7], ZINB loss:1.0574\n",
      "Pretrain epoch [5/7], ZINB loss:1.0904\n",
      "Pretrain epoch [6/7], ZINB loss:1.0948\n",
      "Pretrain epoch [7/7], ZINB loss:1.0712\n",
      "Pretrain epoch [8/7], ZINB loss:1.0445\n",
      "Pretrain epoch [9/7], ZINB loss:1.0959\n",
      "Pretrain epoch [1/8], ZINB loss:1.0354\n",
      "Pretrain epoch [2/8], ZINB loss:1.0629\n",
      "Pretrain epoch [3/8], ZINB loss:1.0358\n",
      "Pretrain epoch [4/8], ZINB loss:0.9683\n",
      "Pretrain epoch [5/8], ZINB loss:1.0318\n",
      "Pretrain epoch [6/8], ZINB loss:1.0713\n",
      "Pretrain epoch [7/8], ZINB loss:1.0367\n",
      "Pretrain epoch [8/8], ZINB loss:1.0543\n",
      "Pretrain epoch [9/8], ZINB loss:0.9560\n",
      "Pretrain epoch [1/9], ZINB loss:1.1113\n",
      "Pretrain epoch [2/9], ZINB loss:0.9855\n",
      "Pretrain epoch [3/9], ZINB loss:0.9987\n",
      "Pretrain epoch [4/9], ZINB loss:0.9848\n",
      "Pretrain epoch [5/9], ZINB loss:0.9831\n",
      "Pretrain epoch [6/9], ZINB loss:0.9611\n",
      "Pretrain epoch [7/9], ZINB loss:0.9979\n",
      "Pretrain epoch [8/9], ZINB loss:0.9699\n",
      "Pretrain epoch [9/9], ZINB loss:0.9915\n",
      "Pretrain epoch [1/10], ZINB loss:0.9729\n",
      "Pretrain epoch [2/10], ZINB loss:0.9835\n",
      "Pretrain epoch [3/10], ZINB loss:0.9615\n",
      "Pretrain epoch [4/10], ZINB loss:0.9659\n",
      "Pretrain epoch [5/10], ZINB loss:0.9730\n",
      "Pretrain epoch [6/10], ZINB loss:0.9381\n",
      "Pretrain epoch [7/10], ZINB loss:0.9796\n",
      "Pretrain epoch [8/10], ZINB loss:1.0245\n",
      "Pretrain epoch [9/10], ZINB loss:0.9257\n",
      "Midtrain epoch [1/11], ZINB loss:1.0036,  neighbor loss 1:-1.6321, expanded neighbor loss 1:-1.4330, self loss:-0.5673\n",
      "Midtrain epoch [2/11], ZINB loss:0.8814,  neighbor loss 1:-1.2631, expanded neighbor loss 1:-1.4574, self loss:-0.5672\n",
      "Midtrain epoch [3/11], ZINB loss:0.9509,  neighbor loss 1:-1.5396, expanded neighbor loss 1:-1.5213, self loss:-0.6155\n",
      "Midtrain epoch [4/11], ZINB loss:0.9698,  neighbor loss 1:-1.7002, expanded neighbor loss 1:-1.5802, self loss:-0.6434\n",
      "Midtrain epoch [5/11], ZINB loss:0.9541,  neighbor loss 1:-1.8809, expanded neighbor loss 1:-1.6605, self loss:-0.6772\n",
      "Midtrain epoch [6/11], ZINB loss:0.9583,  neighbor loss 1:-1.7994, expanded neighbor loss 1:-1.7257, self loss:-0.6956\n",
      "Midtrain epoch [7/11], ZINB loss:0.9606,  neighbor loss 1:-1.6998, expanded neighbor loss 1:-1.7146, self loss:-0.6805\n",
      "Midtrain epoch [8/11], ZINB loss:0.9365,  neighbor loss 1:-1.8617, expanded neighbor loss 1:-1.7887, self loss:-0.7074\n",
      "Midtrain epoch [9/11], ZINB loss:0.9638,  neighbor loss 1:-2.0375, expanded neighbor loss 1:-1.7611, self loss:-0.6998\n",
      "current error: tensor(0.0438, device='cuda:0')\n",
      "Midtrain epoch [1/12], ZINB loss:0.9505,  neighbor loss 1:-1.9716, expanded neighbor loss 1:-1.7822, self loss:-0.7066\n",
      "Midtrain epoch [2/12], ZINB loss:0.9777,  neighbor loss 1:-2.1347, expanded neighbor loss 1:-1.8314, self loss:-0.7244\n",
      "Midtrain epoch [3/12], ZINB loss:0.9257,  neighbor loss 1:-1.9269, expanded neighbor loss 1:-1.8063, self loss:-0.7115\n",
      "Midtrain epoch [4/12], ZINB loss:0.9216,  neighbor loss 1:-2.0745, expanded neighbor loss 1:-1.8560, self loss:-0.7331\n",
      "Midtrain epoch [5/12], ZINB loss:0.9569,  neighbor loss 1:-1.9395, expanded neighbor loss 1:-1.8244, self loss:-0.7242\n",
      "Midtrain epoch [6/12], ZINB loss:0.9833,  neighbor loss 1:-2.1022, expanded neighbor loss 1:-1.8601, self loss:-0.7343\n",
      "Midtrain epoch [7/12], ZINB loss:0.9100,  neighbor loss 1:-2.0800, expanded neighbor loss 1:-1.8927, self loss:-0.7466\n",
      "Midtrain epoch [8/12], ZINB loss:0.9049,  neighbor loss 1:-2.0473, expanded neighbor loss 1:-1.9054, self loss:-0.7515\n",
      "Midtrain epoch [9/12], ZINB loss:0.9205,  neighbor loss 1:-2.0634, expanded neighbor loss 1:-1.9078, self loss:-0.7532\n",
      "current error: tensor(0.0219, device='cuda:0')\n",
      "Midtrain epoch [1/13], ZINB loss:0.9607,  neighbor loss 1:-2.1523, expanded neighbor loss 1:-1.9291, self loss:-0.7663\n",
      "Midtrain epoch [2/13], ZINB loss:0.9322,  neighbor loss 1:-1.9603, expanded neighbor loss 1:-1.8989, self loss:-0.7521\n",
      "Midtrain epoch [3/13], ZINB loss:0.9625,  neighbor loss 1:-2.0568, expanded neighbor loss 1:-1.8930, self loss:-0.7434\n",
      "Midtrain epoch [4/13], ZINB loss:0.8554,  neighbor loss 1:-1.8743, expanded neighbor loss 1:-1.8907, self loss:-0.7369\n",
      "Midtrain epoch [5/13], ZINB loss:0.9297,  neighbor loss 1:-2.2637, expanded neighbor loss 1:-1.8928, self loss:-0.7459\n",
      "Midtrain epoch [6/13], ZINB loss:0.9198,  neighbor loss 1:-2.3019, expanded neighbor loss 1:-1.9123, self loss:-0.7543\n",
      "Midtrain epoch [7/13], ZINB loss:0.9357,  neighbor loss 1:-2.1966, expanded neighbor loss 1:-1.9228, self loss:-0.7603\n",
      "Midtrain epoch [8/13], ZINB loss:0.9455,  neighbor loss 1:-2.2417, expanded neighbor loss 1:-1.9073, self loss:-0.7551\n",
      "Midtrain epoch [9/13], ZINB loss:0.9455,  neighbor loss 1:-2.3679, expanded neighbor loss 1:-1.9572, self loss:-0.7744\n",
      "current error: tensor(0.0079, device='cuda:0')\n",
      "Midtrain epoch [1/14], ZINB loss:0.9902,  neighbor loss 1:-2.2624, expanded neighbor loss 1:-1.9298, self loss:-0.7614\n",
      "Midtrain epoch [2/14], ZINB loss:0.9118,  neighbor loss 1:-2.1480, expanded neighbor loss 1:-1.9697, self loss:-0.7766\n",
      "Midtrain epoch [3/14], ZINB loss:0.8847,  neighbor loss 1:-2.1563, expanded neighbor loss 1:-1.9431, self loss:-0.7636\n",
      "Midtrain epoch [4/14], ZINB loss:0.9227,  neighbor loss 1:-1.9906, expanded neighbor loss 1:-1.8860, self loss:-0.7428\n",
      "Midtrain epoch [5/14], ZINB loss:0.8944,  neighbor loss 1:-2.2321, expanded neighbor loss 1:-1.9729, self loss:-0.7793\n",
      "Midtrain epoch [6/14], ZINB loss:0.9333,  neighbor loss 1:-2.3065, expanded neighbor loss 1:-1.9506, self loss:-0.7717\n",
      "Midtrain epoch [7/14], ZINB loss:0.9327,  neighbor loss 1:-2.3403, expanded neighbor loss 1:-1.9473, self loss:-0.7685\n",
      "Midtrain epoch [8/14], ZINB loss:0.8919,  neighbor loss 1:-2.1477, expanded neighbor loss 1:-1.9685, self loss:-0.7808\n",
      "Midtrain epoch [9/14], ZINB loss:0.9149,  neighbor loss 1:-1.9456, expanded neighbor loss 1:-1.9409, self loss:-0.7624\n",
      "current error: tensor(0.0048, device='cuda:0')\n",
      "Midtrain epoch [1/15], ZINB loss:0.9272,  neighbor loss 1:-2.1676, expanded neighbor loss 1:-1.9748, self loss:-0.7848\n",
      "Midtrain epoch [2/15], ZINB loss:0.9048,  neighbor loss 1:-1.8991, expanded neighbor loss 1:-1.9506, self loss:-0.7679\n",
      "Midtrain epoch [3/15], ZINB loss:0.9063,  neighbor loss 1:-2.0175, expanded neighbor loss 1:-1.9582, self loss:-0.7709\n",
      "Midtrain epoch [4/15], ZINB loss:0.9077,  neighbor loss 1:-2.1466, expanded neighbor loss 1:-1.9737, self loss:-0.7785\n",
      "Midtrain epoch [5/15], ZINB loss:0.9027,  neighbor loss 1:-2.0252, expanded neighbor loss 1:-1.9568, self loss:-0.7721\n",
      "Midtrain epoch [6/15], ZINB loss:0.9229,  neighbor loss 1:-1.9591, expanded neighbor loss 1:-1.9061, self loss:-0.7496\n",
      "Midtrain epoch [7/15], ZINB loss:0.9230,  neighbor loss 1:-2.1363, expanded neighbor loss 1:-1.9932, self loss:-0.7911\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Midtrain epoch [8/15], ZINB loss:0.8891,  neighbor loss 1:-2.1462, expanded neighbor loss 1:-1.9792, self loss:-0.7795\n",
      "Midtrain epoch [9/15], ZINB loss:0.9042,  neighbor loss 1:-2.0226, expanded neighbor loss 1:-1.9711, self loss:-0.7759\n",
      "current error: tensor(0.0013, device='cuda:0')\n",
      "Midtrain epoch [1/16], ZINB loss:0.9032,  neighbor loss 1:-2.0330, expanded neighbor loss 1:-1.9797, self loss:-0.7830\n",
      "Midtrain epoch [2/16], ZINB loss:0.8630,  neighbor loss 1:-1.8658, expanded neighbor loss 1:-1.9835, self loss:-0.7815\n",
      "Midtrain epoch [3/16], ZINB loss:0.9279,  neighbor loss 1:-2.0561, expanded neighbor loss 1:-1.9633, self loss:-0.7736\n",
      "Midtrain epoch [4/16], ZINB loss:0.8702,  neighbor loss 1:-2.0816, expanded neighbor loss 1:-1.9748, self loss:-0.7772\n",
      "Midtrain epoch [5/16], ZINB loss:0.9158,  neighbor loss 1:-2.3169, expanded neighbor loss 1:-1.9767, self loss:-0.7815\n",
      "Midtrain epoch [6/16], ZINB loss:0.9507,  neighbor loss 1:-2.1729, expanded neighbor loss 1:-1.9637, self loss:-0.7759\n",
      "Midtrain epoch [7/16], ZINB loss:0.9076,  neighbor loss 1:-2.4777, expanded neighbor loss 1:-2.0050, self loss:-0.7991\n",
      "Midtrain epoch [8/16], ZINB loss:0.9043,  neighbor loss 1:-2.0948, expanded neighbor loss 1:-1.9563, self loss:-0.7710\n",
      "Midtrain epoch [9/16], ZINB loss:0.8702,  neighbor loss 1:-2.1610, expanded neighbor loss 1:-1.9891, self loss:-0.7846\n",
      "current error: tensor(0.0009, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "bimodality,pred_celltype = model.fit(x=X, annotation=source_annotation, X_raw=count_X,\n",
    "                           size_factor=adata.obs.size_factors,pretrain_epoch=10,midtrain_epoch=20,\n",
    "                                     K=5, KK=5, alpha=0.1)\n",
    "time_cost = time() - t0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1639a197",
   "metadata": {},
   "source": [
    "### calculate annotation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "599e406d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "novel cell types exist: False\n",
      "target cell types: ['acinar' 'alpha' 'beta' 'delta' 'ductal' 'mesenchymal']\n",
      "    Dataset      Total accuracy      Time consuming\n",
      "0  Pancreas  0.9614373356704645  12.809916734695435\n"
     ]
    }
   ],
   "source": [
    "print(\"novel cell types exist:\",bimodality)\n",
    "cellname = np.array(adata.obs[\"celltype\"])\n",
    "print(\"target cell types:\",np.unique(cellname))\n",
    "accuracy=np.mean(pred_celltype == cellname)\n",
    "result=np.array([[dataset,accuracy,time_cost]])\n",
    "output = pd.DataFrame(result,columns=[\"Dataset\",\"Total accuracy\",\"Time consuming\"])\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
